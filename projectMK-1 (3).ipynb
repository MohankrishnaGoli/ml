{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfa24e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Ignore harmless warnings \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set to display all the columns in dataset\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Import psql to run queries \n",
    "\n",
    "import pandasql as psql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60339823",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Dlc\\\\Downloads\\\\bs140513_032310.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19580\\1238050781.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpfdata\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\Dlc\\Downloads\\bs140513_032310.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Copy to back-uo file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpfdata_bk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpfdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Dlc\\\\Downloads\\\\bs140513_032310.csv'"
     ]
    }
   ],
   "source": [
    "pfdata= pd.read_csv(r\"C:\\Users\\Dlc\\Downloads\\bs140513_032310.csv\", header=0)\n",
    "\n",
    "# Copy to back-uo file\n",
    "\n",
    "pfdata_bk = pfdata.copy()\n",
    "\n",
    "# Display first 5 records\n",
    "\n",
    "pfdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a998dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa4934",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfdata.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eeaff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the duplicate values in data set\n",
    "pfdata_dup=pfdata[pfdata.duplicated(keep='last')]\n",
    "pfdata_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e0550",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfdata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2fa37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfdata['fraud'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a54b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfdata['zipcodeOri'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc2e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfdata['merchant'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1d6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfdata['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3b5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfdata['zipMerchant'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da49cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfdata['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "LE=LabelEncoder()\n",
    "pfdata['gender']=LE.fit_transform(pfdata['gender'])\n",
    "pfdata['category']=LE.fit_transform(pfdata['category'])\n",
    "pfdata['age']=LE.fit_transform(pfdata['age'])\n",
    "pfdata['merchant']=LE.fit_transform(pfdata['merchant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28473c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfdata['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81607b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb85e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_count=pfdata.fraud.value_counts()\n",
    "print('class 0:',fraud_count[0])\n",
    "print('class 1:',fraud_count[1])\n",
    "print('proportion:',round(fraud_count[0]/fraud_count[1],2),':1')\n",
    "print('Total fraud Modelling records:',len(pfdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c6efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfdata=pfdata.drop(['zipcodeOri','customer','zipMerchant'],axis=1)\n",
    "pfdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45450228",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee864fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Indepvar=[]\n",
    "for col in pfdata.columns:\n",
    "      if col !='fraud':\n",
    "            Indepvar.append(col)\n",
    "TargetVar='fraud'\n",
    "x=pfdata[Indepvar]\n",
    "y=pfdata[TargetVar]\n",
    "x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78572945",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0863d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random oversampling can be implemented using the RandomOverSampler class\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy=0.125)\n",
    "\n",
    "x_over, y_over = oversample.fit_resample(x, y)\n",
    "print(x_over.shape)\n",
    "print(y_over.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1bfc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test (random sampling)\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Display the shape for train & test data\n",
    "\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307526cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features by using MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mmscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "x_train = mmscaler.fit_transform(x_train)\n",
    "x_train = pd.DataFrame(x_train)\n",
    "\n",
    "x_test = mmscaler.fit_transform(x_test)\n",
    "x_test = pd.DataFrame(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e46c8",
   "metadata": {},
   "source": [
    "# KNN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1630f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_Results = pd.read_csv(r'C:\\Users\\Dlc\\Downloads\\KNN_Results.csv',header=0)\n",
    "KNN_Results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a9e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bild KNN Model\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for a in range(1, 21, 1):\n",
    "    \n",
    "    k = a\n",
    "    \n",
    "    # Build the model\n",
    "    \n",
    "    ModelKNN = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    # Train the model\n",
    "    \n",
    "    ModelKNN.fit(x_train, y_train)\n",
    "    \n",
    "    # Predict the model\n",
    "    \n",
    "    y_pred = ModelKNN.predict(x_test)\n",
    "    y_pred_prob = ModelKNN.predict_proba(x_test)\n",
    "    \n",
    "    print('KNN_K_value = ', a)\n",
    "    \n",
    "    # Print the model name\n",
    "    \n",
    "    print('Model Name: ', ModelKNN)\n",
    "    \n",
    "    # confusion matrix in sklearn\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    # actual values\n",
    "    \n",
    "    actual = y_test\n",
    "    \n",
    "    # predicted values\n",
    "    \n",
    "    predicted = y_pred\n",
    "    \n",
    "    # confusion matrix\n",
    "    \n",
    "    matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)\n",
    "    print('Confusion matrix : \\n', matrix)\n",
    "    \n",
    "    # outcome values order in sklearn\n",
    "    \n",
    "    tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "    print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "    \n",
    "    # classification report for precision, recall f1-score and accuracy\n",
    "    \n",
    "    C_Report = classification_report(actual,predicted,labels=[1,0])\n",
    "    \n",
    "    print('Classification report : \\n', C_Report)\n",
    "    \n",
    "    # calculating the metrics\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3);\n",
    "    specificity = round(tn/(tn+fp), 3);\n",
    "    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "    \n",
    "    precision = round(tp/(tp+fp), 3);\n",
    "    f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "    \n",
    "    # Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "    # A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "    \n",
    "    from math import sqrt\n",
    "    \n",
    "    mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "    \n",
    "    print('Accuracy :', round(accuracy*100, 2),'%')\n",
    "    print('Precision :', round(precision*100, 2),'%')\n",
    "    print('Recall :', round(sensitivity*100,2), '%')\n",
    "    print('F1 Score :', f1Score)\n",
    "    print('Specificity or True Negative Rate :', round(specificity*100,2), '%'  )\n",
    "    print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')\n",
    "    print('MCC :', MCC)\n",
    "    \n",
    "    # Area under ROC curve \n",
    "    \n",
    "    from sklearn.metrics import roc_curve, roc_auc_score\n",
    "    \n",
    "    print('roc_auc_score:', round(roc_auc_score(actual, predicted), 3))\n",
    "    \n",
    "    # ROC Curve\n",
    "    \n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    model_roc_auc = roc_auc_score(actual, predicted)\n",
    "    fpr, tpr, thresholds = roc_curve(actual, ModelKNN.predict_proba(x_test)[:,1])\n",
    "    plt.figure()\n",
    "    # plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "    plt.plot(fpr, tpr, label= 'Classification Model' % model_roc_auc)\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    #plt.savefig('Log_ROC')\n",
    "    plt.show()\n",
    "    #------------------------------------------------------------------------------\n",
    "    new_row = {'Model Name' : ModelKNN,\n",
    "               'KNN K Value' : a,\n",
    "               'True_Positive' : tp,\n",
    "               'False_Negative' : fn,\n",
    "               'False_Positive' : fp,\n",
    "               'True_Negative' : tn,\n",
    "               'Accuracy' : accuracy,\n",
    "               'Precision' : precision,\n",
    "               'Recall' : sensitivity,\n",
    "               'F1 Score' : f1Score,\n",
    "               'Specificity' : specificity,\n",
    "               'MCC':MCC,\n",
    "               'ROC_AUC_Score':roc_auc_score(actual, predicted),\n",
    "               'Balanced Accuracy':balanced_accuracy}\n",
    "    KNN_Results = KNN_Results.append(new_row, ignore_index=True)\n",
    "    #------KNN_Results------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b4f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb54b571",
   "metadata": {},
   "source": [
    "# SVM linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a895a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMResults1= pd.read_csv(r'C:\\Users\\Dlc\\Downloads\\EMResults.csv',header=0)\n",
    "EMResults1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6237dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training the SVM algorithm with train dataset\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "ModelSVM1 = SVC(C=1.0, kernel='linear', degree=3, gamma='scale', coef0=0.0, shrinking=True, \n",
    "                probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False, \n",
    "                max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
    "\n",
    "# Train the model with train data \n",
    "\n",
    "ModelSVM1 = ModelSVM1.fit(x_train, y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y_pred = ModelSVM1.predict(x_test)\n",
    "y_pred_prob = ModelSVM1.predict_proba(x_test)\n",
    "\n",
    "# Print the model name\n",
    "    \n",
    "print('Model Name: ', \"SVM - Linear\")\n",
    "\n",
    "# Confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "\n",
    "predicted = y_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "C_Report = classification_report(actual,predicted,labels=[1,0])\n",
    "\n",
    "print('Classification report : \\n', C_Report)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3);\n",
    "specificity = round(tn/(tn+fp), 3);\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "precision = round(tp/(tp+fp), 3);\n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%')\n",
    "print('Precision :', round(precision*100, 2),'%')\n",
    "print('Recall :', round(sensitivity*100,2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Specificity or True Negative Rate :', round(specificity*100,2), '%'  )\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')\n",
    "print('MCC :', MCC)\n",
    "\n",
    "# Area under ROC curve \n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "print('roc_auc_score:', round(roc_auc_score(actual, predicted), 3))\n",
    "\n",
    "# ROC Curve\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "model_roc_auc = roc_auc_score(actual, predicted)\n",
    "fpr, tpr, thresholds = roc_curve(actual,ModelSVM1.predict_proba(x_test)[:,1])\n",
    "plt.figure()\n",
    "# plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot(fpr, tpr, label= 'Classification Model' % model_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show() \n",
    "print('-----------------------------------------------------------------------------------------------------')\n",
    "#---\n",
    "new_row = {'Model Name' : \"SVM - Linear\",\n",
    "            'True_Positive' : tp, \n",
    "            'False_Negative' : fn, \n",
    "            'False_Positive' : fp,\n",
    "            'True_Negative' : tn,\n",
    "            'Accuracy' : accuracy,\n",
    "            'Precision' : precision,\n",
    "            'Recall' : sensitivity,\n",
    "            'F1 Score' : f1Score,\n",
    "            'Specificity' : specificity,\n",
    "            'MCC':MCC,\n",
    "            'ROC_AUC_Score':roc_auc_score(actual, predicted),\n",
    "            'Balanced Accuracy':balanced_accuracy}\n",
    "EMResults1 = EMResults1.append(new_row, ignore_index=True)\n",
    "#-------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2da9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6f3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c280707",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ec5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMResults = pd.read_csv(r'C:\\Users\\Dlc\\Downloads\\EMResults.csv',header=0)\n",
    "EMResults.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c856ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Calssification models and compare the results\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create objects of classification algorithms with default hyper-parameters\n",
    "\n",
    "ModelLR = LogisticRegression()\n",
    "ModelDC = DecisionTreeClassifier()\n",
    "ModelRF = RandomForestClassifier()\n",
    "ModelET = ExtraTreesClassifier()\n",
    "ModelKNN = KNeighborsClassifier(n_neighbors=2)\n",
    "ModelGNB = GaussianNB()\n",
    "ModelSVM = SVC(probability=True)\n",
    "\n",
    "# Evalution matrix for all the algorithms\n",
    "\n",
    "MM = [ModelLR, ModelDC, ModelRF, ModelET, ModelKNN, ModelGNB, ModelSVM]\n",
    "#MM = [ModelLR, ModelDC, ModelRF, ModelET,ModelKNN]\n",
    "\n",
    "for models in MM:\n",
    "            \n",
    "    # Train the model training dataset\n",
    "    \n",
    "    models.fit(x_train, y_train)\n",
    "    \n",
    "    # Prediction the model with test dataset\n",
    "    \n",
    "    y_pred = models.predict(x_test)\n",
    "    y_pred_prob = models.predict_proba(x_test)\n",
    "    \n",
    "    # Print the model name\n",
    "    \n",
    "    print('Model Name: ', models)\n",
    "    \n",
    "    # confusion matrix in sklearn\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    # actual values\n",
    "\n",
    "    actual = y_test\n",
    "\n",
    "    # predicted values\n",
    "\n",
    "    predicted = y_pred\n",
    "\n",
    "    # confusion matrix\n",
    "\n",
    "    matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)\n",
    "    print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "    # outcome values order in sklearn\n",
    "\n",
    "    tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "    print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "    # classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "    C_Report = classification_report(actual,predicted,labels=[1,0])\n",
    "\n",
    "    print('Classification report : \\n', C_Report)\n",
    "\n",
    "    # calculating the metrics\n",
    "\n",
    "    sensitivity = round(tp/(tp+fn), 3);\n",
    "    specificity = round(tn/(tn+fp), 3);\n",
    "    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "    \n",
    "    precision = round(tp/(tp+fp), 3);\n",
    "    f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "    # Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "    # A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "    from math import sqrt\n",
    "\n",
    "    mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "\n",
    "    print('Accuracy :', round(accuracy*100, 2),'%')\n",
    "    print('Precision :', round(precision*100, 2),'%')\n",
    "    print('Recall :', round(sensitivity*100,2), '%')\n",
    "    print('F1 Score :', f1Score)\n",
    "    print('Specificity or True Negative Rate :', round(specificity*100,2), '%'  )\n",
    "    print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')\n",
    "    print('MCC :', MCC)\n",
    "\n",
    "    # Area under ROC curve \n",
    "\n",
    "    from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "    print('roc_auc_score:', round(roc_auc_score(actual, predicted), 3))\n",
    "    \n",
    "    # ROC Curve\n",
    "    \n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    Model_roc_auc = roc_auc_score(actual, predicted)\n",
    "    fpr, tpr, thresholds = roc_curve(actual, models.predict_proba(x_test)[:,1])\n",
    "    plt.figure()\n",
    "    #\n",
    "    plt.plot(fpr, tpr, label= 'Classification Model' % Model_roc_auc)\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('Log_ROC')\n",
    "    plt.show()\n",
    "    print('-----------------------------------------------------------------------------------------------------')\n",
    "    #----------------------------------------------------------------------------------------------------------\n",
    "    new_row = {'Model Name' : models,\n",
    "               'True_Positive': tp,\n",
    "               'False_Negative': fn, \n",
    "               'False_Positive': fp, \n",
    "               'True_Negative': tn,\n",
    "               'Accuracy' : accuracy,\n",
    "               'Precision' : precision,\n",
    "               'Recall' : sensitivity,\n",
    "               'F1 Score' : f1Score,\n",
    "               'Specificity' : specificity,\n",
    "               'MCC':MCC,\n",
    "               'ROC_AUC_Score':roc_auc_score(actual, predicted),\n",
    "               'Balanced Accuracy':balanced_accuracy}\n",
    "    EMResults = EMResults.append(new_row, ignore_index=True)\n",
    "    #----------------------------------------------------------------------------------------------------------\n",
    "#======================================================================================================================>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predRF=ModelRF.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df4582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = pd.DataFrame({'fraud_A':y_test,'fraud_P':y_pred})\n",
    "\n",
    "ResultFinal = pfdata_bk.merge(Results,left_index=True,right_index=True)\n",
    "ResultFinal.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c046f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
